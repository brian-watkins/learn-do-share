
# Learn, Do, Share

I’m building an app that will at least in part run in the browser, and I want to be able to share some code between the front end and the back end. So Javascript is probably the easiest option (without learning new tools). But I also want fast feedback and confidence that things are syntactically correct, so I will use Typescript. 

I’m a little worried that I’m going to have server-side code and client-side code mixed together and this will require difference Typescript configs (for different module resolution strategies). Maybe using esmodules will help with this but we’ll see.

So I installed Typescript and my preferred test framework: esbehavior.

First I wrote a test that just fails to make sure my testing framework is working.

### The first test

I think my first releasable product is something that can display a list of learning areas. Now, this could be done with just static HTML. But I’d like to be able to change the learning areas dynamically, without redeploying the app. So, I’ll need to be able to run a web app that fetches content (a list of learning areas) (from somewhere) and displays it.

That’s a lot of stuff to get going — basically the entire app. So for the first test let’s start smaller. We’re describing the behavior around viewing learning areas. Our first example will describe what happens when no learning areas can be found — not an error case, just when we (successfully) find no learning areas. We’ll assert that we see a message saying there are no learning areas.

Why? This gets us working software — our app can be deployed and used and basically it says what it can do: not much. Also, it will drive out just /part/ of the system, namely the client-side app. We can figure out how to load the client side app during our test and make assertions about it, without having to worry about the server-side yet — since all we need to do is really get some HTML that displays the message we want.

I will use Playwright to drive a web browser during our tests. Why? it’s fast and it doesn’t lock us into running our tests in Chrome, like Puppeteer. It also gives us more flexibility for running tests in the future, but for right now we’ll just use it to load a web page. But we need to serve that web page during our tests. For that, I’ll use vite. Why? It’s fast; it feels like the new thing; it’s easy to configure (or doesn’t require a lot of configuration); and it works with esmodules which I will want to do as well.

I can start the vite dev server in my runner.ts file before I validate the behaviors.

Right now I start puppeteer at the beginning of each example and shut it down at the end. This will need to change.

### Watch mode

I want to add watch mode to my tests so that the tests will automatically run when changes are made.

It’s easy enough to add an event listener to the underlying FSWatcher instance exposed by vite dev server (from chokidar). But the problem is that this is just watching my application src files; it won’t re-run when my behavior files change. And what’s more, I’m writing my behaviors in typescript so they need to be recompiled to run, and right now that would mean that I would need to restart the entire test runner process.

One option would be to run the tests in a separate process? We can do this, but then the problem is that it will be more difficult to share the reference to the browser. But maybe that’s ok. 

Even if we set this up, it turns out that chokidar will fire off change events multiple times when a file is changed under the root directory. This is annoying. So I’m just going to pass on watch mode for now and can set it up later if it becomes more useful.

### The second test

With the second test, we’re going to assert that given some learning areas, when the app loads, it shows those learning areas. This means, we need some way for our running application to become aware of the learning areas we specify in our example.

There’s many ways to do this of course. Probably the most straightforward would be to replace our index.html — served by vite dev server — with HTML that is generated by our own server. Our server will do whatever is necessary to fetch the data and then can construct the HTML accordingly. On this approach, all the logic for our app (including display logic) lives in one place: the server app. And there’s no API or anything that is exposed other than the routes that a web browser would request for HTML.

The typical ‘jamstack’ alternative would be to continue with vite serving our HTML but add some JS that would fetch the data about learning areas from our server. It would then use some more JS to render HTML based on the response from the server. This strategy seems pretty good. For one, there seems to be a separation of concerns between client and server: the client is in charge of rendering HTML, the server is in charge of fetching, transforming, and, returning data that the client uses. As such, the server /can/ serve multiple front-ends — a mobile app and a web app for instance. In general, breaking things into horizontal layers makes it easier to change the various layers.

I want to avoid creating an API right off the bat. But I also kind of want to avoid simply generating HTML on the server. The question is: What will put us in a position that leaves us with an app that is easier to change?

We know we will need some kind of API because we will need to update data at some point. And when we do that, we don’t want to re-render the whole page anyway. So we will need some way to update things on the client side via JS. 

So I will create an API, but it will have only one endpoint. And that enpoint will accept messages that can be shared between the client and the server, just ensuring that we are in sync. This will make our software easier to change and give us confidence that if it compiles it works.

In any case, however, I will need some sort of database that the server mediates access to — either something like MySQL or a cloud database like CosmosDB or Firestore. So this test will drive out two main things: a persistance tool and a server.

### The Server

So what should I use for the server? Express? Koa? Fastify? I want to say that it kind of doesn’t matter. I’m hoping to deploy the server-side component as a serverless function, so the actual http server is something I won’t have control over. But for testing purposes and running locally, I will need something to serve things, at least for the time being.

I will use express — it’s boring but the most straightforward thing. Koa is annoying to work with, and fastify has a bunch of stuff that we wouldn’t actually use (like schema validation etc).

We will proxy requests via Vite to a test server that we start and stop with each example. We do this so we can have full control over things like …

### The Store

I also need to figure out how I will persist things. But for now, since I don’t actually have to /change/ anything, it’s easy enough just to have a static store that I populate by hand. That way, we can either ship it with data populated by hand or not, and we can delay the decision about the DB for a while longer. Hopefully this will allow us to write code that doesn’t care so much about what our DB turns out to be.

### The test passes

We got the test to pass but probably need to refactor some to make it easier  to understand what’s going on. We added Redux and refactored the server so it can be extended without modification by adding new items to the registry.

### Updating the test

Now we update the test so that we expect a loading indicator to appear while the learning areas are being fetched from the server. Now that we have Redux, we can declare our initial state to specify that we are loading the items from the server. And our view can display a loading indicator if it sees this in the state. When the response is returned, we update the view. 

To get this working, we’ll need to change our FakeLearningAreasReader to resolve whenever we want (instead of immediately). We do this by capturing the resolve function from the promise we return and invoking it later in the test.

We can refactor things a bit more to group all the messages related to LearningAreas in one file. It feels like a lot of code to write so there may be some better patterns here. We also probably need to establish a better pattern for how to add new mesages and new requests. But that can wait until we actually need to do this. Finally, the view is being recalculated each time we render. This is probably fine but at some point we might need to optimize by only updating parts of the view if some part of the state changes. But we should wait until that becomes a problem. Finally, we probably want to refactor our view to give us some better functions to work with, instead of working with the raw Snabbdom primitives.

### Run locally

We still need to be able to run locally. This means:

1. We need to build and run our server locally
	1. This requires us to provide a static learning areas reader
2. We need to run the client locally and have it be able to communicate with the server — we should be able to use vite for this

We created a separate directory with just the entry point for the server and it’s own tsconfig file so that we can control the output directory. We then created another directory for the client app with its entry point (index.html and app.js) — we probably still need to pull out more from app.js to make it just contain the bare minimum.

We also added node foreman and a Procfile to describe the server and client processes and we added a script to run these, waiting for the server to be available before starting the client.

### Build

We want to deploy. But first we need to build …

1. We want something to serve static web content — this is our front end web app, which as of right now is just static js and html. Ie no server-side rendering
2. We want something to run our server app, which currently accepts messages from the frontend in a request/response fashion.

So, the first step here is to build our assets for deploy. So we have a `build` directory with subdirectories for the `display` and the `server` and we use tsc to do type checking before we build for real. And because we add in some other tsconfig.json files we can type check just those files independently of each other by treating the server and the client as separate typescript /projects/.

We use `esbuild` to build our server app which will do any necessary tree shaking to remove code that’s not actually used in the backend. And for the display, we use `vite` to bundle the javascript and do the same thing, removing any code used only on the server.

### Deploy

Where do we deploy?  Fly.io? Azure? We could try multiple places, but let’s start with Azure.

Azure app service apparently lets us deploy a docker container. So that’s probably the easiest. And we can definitely do static web content with the Azure static web app. We could also consider deploying the backend as an azure function.

We should be able to do all this with GitHub actions. There is a `setup-pack` GitHub action that installs pack and other tools so we could definitely use that:

[GitHub - buildpacks/github-actions: End-user GitHub Actions related to Cloud Native Buildpacks](https://github.com/buildpacks/github-actions#setup-pack-cli-action)

Then we need somewhere to push the image. Or maybe Azure takes care of that for us if we have an image locally to push?

Note that for the CNB to build our server into an image, we need to provide a package.json with some details that’s specific to the server, with only the server dependencies etc. As part of the build:server process we should generate that or copy it to the build folder or something.

Note that we /can/ use pack locally on Mac OS — it’s just that it builds an x64 image and runs it under emulation I think with QUEMU I think somehow. 

For the server component: If I build my own container image, I need to store it somewhere. Azure is actually kind of expensive: $0.16/day. I could use docker and store a public image. I could also just give the source code to Azure and let it figure things out. Maybe that is easiest. But turns out that if I create the Azure web service it costs ~$14/month

For the display component: I want to deploy as an Azure Static Web App. And then I want to use a function proxy to send the requests to my server component at `/api`. Now, I could theoretically just include the server component as a function associated with the static web app … 

I think for Azure, the simplest thing is to deploy a static web app along with a function that will do the backend work. This makes it kind of more clear that we are deploying /one thing/. So the build task would result in output that looks a little different … maybe. And I’m not sure how to structure the function code or what needs to be there to get it to run.

We just created a static web app first, and Azure created a GitHub action to build and deploy; we specify the node engine in the package.json now to let the action know the right version of Node to use. Now we can add a function (or a proxy function if we wanted).

Finally got it working. I added a function. Had to add a staticwebapp config file to specify to use node 16 as the runtime. Then had to make sure that the built javascript had an extension of `.mjs` so that azure would know to treat it as an es module. Then it finally worked.

So when I deploy the app uses a function. But when running locally we use express to run the server-side component. One problem is that both the function and the express server have to know what the endpoint path is. And the frontend code also has to know this path too. How can we remedy this so that we get some feedback if we mistakenly change the path? The problem is that for the function, the path is defined by a directory structure and nothing else; so even if we were to define the path in code, we couldn’t refer to that when building the azure function I think. Unless we dynamically wrote out the azure function config files as part of the build process ha ha


### Refactoring

In refactoring the code, I want to basically abstract away anything that’s not part of the app logic itself. For example,

1. I want to abstract away details about the deployment
2. I want to abstract away details about the framework — like Snabbdom and redux and certainly the middleware that we are writing.

What I ended up with is a set of directories:

- `src` contains the application source
- `api` contains details about the server less function that is deployed to azure for the backstage processing. While we could call this directory something else, this is Azure’s default. But inside it, we are restricted to a certain directory structure. This code creates Adapters for this environment and then calls a function to create a `Backstage` object that contains a registry with message names and handlers.
- `local/backstage` contains details about how to run the backstage processing locally and during tests. It creates an express app that processes messages
- `display` contains details about how to run the display. Because we are using `vite` to build the app, we can actually use the `vite` dev server to run locally and for the tests. So there’s no need to have a separate directory with details for running locally.

The application needs to provide a `Program` which contains a `view`, `update`, and `initialCommand` — we might want to change this to a `Display` or something.

The application also needs to provide a `Backstage` which just has one property: a message handler function.

The only other piece of infrastructure we will need is a way to create asynchronous commands. So, when someone clicks a button, we can define the logic of updating the state and sending a backstage message if necessary. Although we’ll have to see — it may just be a matter of passing the backstage message through to the reducer. But for things like periodic events that use setInterval or something … this is basically a message generator like the backstage message but it doesn’t need to go to the backstage and it could potentially produce messages indefinitely — not just once like backstage messages.

Maybe a Backstage message is actually a special case of this. It’s just that in this case, the action — the message producing action — is the same for every backstage message, so it can be abstracted. 

Note that the backstage handler function is the kind of message producer we want; it’s just that it runs on the server side and only returns one message. 

So we generalized the backstage request middleware to introduce the notion of effects. So now it’s possible to register effect handlers, basically. These are associated with a message type. The middleware looks at the incoming message and if there is a message handler for this message type, then the handler is invoked with a dispatcher function so that it can dispatch other messages.

So far this is just used to handle the Backstage message. But in the future, the `Display` abstraction could provide a `Map` from message types to handlers so that the application could define arbitrary effect handlers.

Unfortunately, we’re basically vanity refactoring at this point. We need to move on with some features to prove out the overall design.

### Add Styling

The next thing we want to do is add the ability to style our content. 

We will add Tailwind CSS so that we can use ‘atomic css’ or a ‘utility-first css’ approach.

The benefits of this is that it’s easier to build our consistent style and change things at a global level. Plus our CSS file will be smaller since it only contains the classes that we actually use. And it seems to be a popular choice.


### Interactivity

Next thing I want to do is a story that requires some interactivity on the view. This will help drive out how we handle events. The best option I think is a story to show the content associated with a learning area. We’ll click a learning area and then it will show the content. 

It’s not clear whether we should render the content immediately or whether we should fetch the content when the item is opened. I think we could always make an extra request just for the notes associated with a learning area. We also know that the content to fetch will be relatively constant, since we don’t expect the learning areas to change a ton (although the content could be several paragraphs for each). But the number of comments could grow (although they will be small). We could do other things to optimize anyway — like fetch the content immediately after fetching the titles of the areas or something. It’s probably better in general to see content immediately when clicking a learning area rather than having to wait. Plus it’s a simpler approach.

So we will assume that the content is bundled with the titles of the learning areas.

We did it!

Our view is starting to get a little more complicated so we abstracted some helper functions. 

### The next feature: Rendering Markdown in learning area content

We decided to use the remark/rehype ecosystem as it has tons of ways to transformations to the html generated from markdown. For example, we need to add styles to different element types since we are using atomic css.

This seems to work fine. We wrote our own plugin that actually adds classes to various elements so we can style it like we want to. Of course, we need to add styles for every tag that could possibly show up, but we know the content for the learning areas and can ensure that it looks fine.

But we realized that … we aren’t running our tests in CI!!!

### Running Tests in CI

This was easy enough to do. And in fact it turns out that the linux container in GitHub actions already has all the dependencies necessary to run playwright chromium … maybe that’s just an accident somehow but it seems to work. Plus there’s a playwright cli command we can use to install dependencies if we need to. 

We also validate our types before running the tests, and only build and deploy if the tests pass. We have to do a separate step for type validation because we are using esbuild during development to speed things up — and esbuild does not do any type checking.

### Refactoring

Now we’ll take some time to refactor some things.

One thing I’d like to do is try to add Immer so that my update function can be a little cleaner. We should also try to reorganize the code a bit, as the `learningAreas.ts` file is getting a little large (and has some details about how we parse markdown into HTML that we might want to abstract away somehow)

We added immer — and because we are abstracting away a lot of the details it was easy to add at a higher level (where we create our reducer function) and just simplify the function that the application display needs to provide to update the state when messages come in.

### Another markdown story

We’ll actually do the refactor of the markdown code as part of this next story. The story itself should be very easy — we might even just write our own transformer, or there exists one that adds target to link tags. And then we can refactor the markdown stuff to make it clearer what’s happening.

First we write a test. We now need to do things like fetch the attribute of an element, and not only that, we need to be able to assert about attributes of *all* elements on a page matching a certain selector (to see if all links have the proper attributes). So it feels like a good time to refactor our we deal with elements a bit.

We will create a new abstraction `DisplayElement` that contains helper methods. And we’ll add some functions to either select one or many display elements based on a selector string.

We’ll need to update some of the other tests, but this should make it easier to write new tests in the future — we have a more flexible api now for finding and asserting on elements of the display.

Now that we have a failing test we need to figure out how to add the attributes to the `a` tag. We start off with something generic but then decide to just write something that adds target and rel specifically to `a` tags. This is easy, and now the test passes.

Now we refactor some. We wanted to move the markdown code into its own module since it’s getting bigger and doesn’t really have much to do with the learningArea module that it’s currently in. So we create a `MarkdownParser` module and put it in there. BUT we want to be able to pass in relevant data about the styling etc for the html elements in the markdown. Really what we are doing is just adding attributes to tags — both when we add classes and when we add target and rel to links. So now’s the time to make something generic (ie after our test is green). And since we’ve written both implementations, it’s easy to see how to make this generic. We implement it with a kind of complicated config object, but then create a helper function to help generate the config object. Finally we refactor the names from `TagAdditions` to `TagDecorator` which seems to fit better. We clean things up and then we’re done.

We pushed the change but the build failed because there were unused imports, which failed the tsc validation step. We can run `validate` locally to catch this but we usually just run the tests. We need something like a git hook to run validate and test before we push.

### Adding a hook to validate and run tests on push? or commit?

Decided to use Husky to manage git hooks. Why? Because otherwise we couldn’t check in the git hook into source control. Even if we manged the script ourselves and set the git config value to find hooks in a different directory, we’d still need to write some script or something to set that up when you pull the repo. At least Husky handles that for us. 

One problem though, GitX doesn’t run git hooks if the git hooks folder location is changed! Maybe we can build a version of GitX that fixes this …

### Persistence

We’ll tackle persistence before we handle authentication, since authentication doesn’t have a ton of value until we can actually save things. We want our learning areas to be public after all.

So for persistence, the question is: what do we really need? We know that we want to store the learning areas and their content (eventually). We also will need to store, on a per user basis, which items that user is learning, doing, or sharing. And we’ll nede to store notes for each learning item. I think that’s it. 

When a person logs in, then we will need to fetch the items that they are engaged with. And when they click on an area, we will need to load the number of notes. I don’t think we need to show the number of notes or anything like that. 

So if we use a document store, we could fetch one document based on the user id when we log in. This has a list of ids that the person is engaged with. Then we fetch Note documents based on the learning area id and the user id whenever a learning area is opened. 

So, since we are in Azure, we could try to use Cosmos DB. I’d like to try and work with that anyway. 

But persistence is a heavy thing to worry about and we actually have a kind of complicated interaction to do on the UI anyway. So, we’ll ‘persist’ in memory at first, and then worry about integrating with a data store. ACTUALLY, it turns out we can’t really do this since our backend is a serverless function … we can’t expect it to share memory from one request to another (although it might sometimes).

### Engagement level feature

For this feature, we need something like a select list or a radio group or something to be able to indicate the level of engagement with a particular learning area. We chose a radio group so that the user can see all the options at once.

There’s several things to consider here.

1. When we select an engagement level, and close the area, then we should see a little tag that indicates the engagement level.
2. If we select ‘Not interested’ then that tag should disappear
3. When we select an engagement level, a request should be made to persist this backstage.
4. When we restart the app, we should see the expected engagement level for the selected learning area.

This needs to happen for all the different engagement levels. We could have a shared behavior to do this. Or we could have one long example with three or four areas which selects for each engagement level on one. The second option is probably better since it will allow us to describe the loading behavior with multiple areas, each with different levels of engagement.

We’re going to need to create a database and containers in cosmos db … we need someway to track this so we should probably be using terraform (I’m not sure how otherwise Cosmos DB is supposed to work since it doesn’t really have a schema or anything). but that’s even more stuff to set up. I guess for now, we can create things through the UI and then use terraform later.

So we created a Cosmos DB and connected to it. And it seems to work just fine. But there are several things we need to do:

1. Externalize the config into environment variables for when we deploy [DONE]
2. Make it so that the database can only be accessed from the backstage function [DONE]
3. Use a Fake Cosmos DB during tests so we can exercise the code but not have to make requests to an actual instance [DONE]
4. Figure out what we want to do when we run locally. Can we use a fake? Do we need another instance of the DB? or a separate DB in our one Azure resource? [DONE]
5. Create terraform scripts to create the Cosmos DB resource at least (we will use the `createIfNotExists` function to make the database and the containers when we need them)


NOTES
- Can we do a better job selecting display elements in the test, so we can hide the details like the :has-text selector? We would need to be able to select descendants basically
	- Yes — we can use locator.locator(selector) to do this I think …
	- DONE
- The actions portion of the Display has a weird type signature or could otherwise be better I think. [DONE — removed it and replaced with batch messages]
- Create an enum for engagement levels for better type safety [DONE]


### Terraform

We want to manage the infrastructure we are building with Azure. So we should use terraform. We created a basic terraform script that just creates our resource group. Now we need to do the other parts.

Unfortunately, Terraform has kind of weak support for azure static web apps configuration. For example, it’s not clear how to associate Azure App Insights or how to specify the git repo or how to set Application Parameters (which we use to pass the db connection to the function).

For some of this stuff, maybe we can use a resource group template deployment as described here: [Support for app settings configuration for azurerm_static_site · Issue #13451 · hashicorp/terraform-provider-azurerm · GitHub](https://github.com/hashicorp/terraform-provider-azurerm/issues/13451)

But looks like we need to build the App Insights resource first maybe, then the Cosmos DB resource, and then use the values we get from that when we create the static web app somehow.

Also, we will still need to get an `api_key` from the static web app to add to our github workflow manually I think, as a secret. We can actually use terraform to add the api_key secret maybe … although that means we need to authenticate with github when we run terraform (as well as Azure) …

We can actually use an Azure Blob Store Blob to store the terraform state. And that would allow us to run it from github actions. That was pretty easy and now we store our state in the Azure Blob instead of locally. This will make it easier to run terraform from github actions. And once we’re in github actions then it will be super easy to use the github provider to create a secret for our deploy github action based on the static web app that was deployed by terraform.

To create a github action to run terraform, all we needed to do is use the `setup-terraform` action and set some environment variables. We created a service principal that is a contributor in our resource group. We also had to add that service principal as a contributor to the `lds-terraform` blob also so it could update the terraform state as needed.

Now we just need to actually apply changes automatically. We will update the terraform so that it creates a github secret automatically based on the api key from the static web app — so that the ci/cd github workflow can deploy the static webapp.

But it would be good to have some way of approving what it is that will happen. Maybe we have it somehow create a pull request with the contents of `terraform plan` and then when we approve that it actually applies the changes? But then would we have to remember to always make changes on some other branch or something?

We got terraform creating a github actions secret during the github actions workflow … but we had to create a Personal Access Token for terraform to use to authenticate with the github provider instead of using the built-in actions GITHUB_TOKEN .. seems like the github token just doesn’t have the write scopes even though it says it does. 

So now all the terraform works and will automatically run apply any time we make a change to the terraform scripts. This is probably bad in that we probably want to confirm that it’s doing the right thing. Can we disallow commits to main for specific files? Maybe I should have a develop branch? And I could auto-approve pull requests to main except for changes that involve the terraform branch? For now maybe it’s fine so long as I can run `terraform plan` locally for testing. Now I have a script called `plan.sh` in the `terraform` directory that will run plan with the proper credentials (assuming you’ve logged into azure). That seems sufficient for now.

### Tracing via Application Insights

We also added the application insights JS library to the display so that we can trace fetch requests to backstage. We can now see this in the application map in application insights which is cool. Unfortunately, we’re not able to see the time spent in cosmos db (unless we were to instrument this manually) because the JS library for app insights apparently does not yet support the cosmos db sdk.

### Server-side rendering

The goal is to have the page load immediately with the learning areas, so there’s no `loading ...` screen. We don’t want to build this data in at build time because we want to be able to update the learning areas whenever we want without having to do a new deploy. (Maybe that’s dumb though). 

So in order to have the `index.html` page be dynamic, we probably need to change our architecture a lot. The static assets should be hosted in Azure blob storage. This `index.html` page should be hosted by a function. And then we would need another function for backstage. Maybe it’s a function app that serves both functions from the same domain.

There’s actually a few ways to do this:

1. Dynamically generate the `index.html` on reach request, render necessary HTML.
2. Dynamically generate the `index.html` on each request, set the initial state and let snabbdom render HTML in the browser based on that.
3. Generate `index.html` at build time, render necessary HTML based on request to database.
4. Generate `index.html` at build time, set initial state with javascript, let snabbdom render HTML when the page is loaded in the browser.

The issue with (1) and (3) is that we have to ‘hydrate’ the HTML so that snabbdom then knows about it and can take it over in its virtual dom. So probably what really has to happen is if we do (1) we have to do (2) also immediately after the page loads. 

And right now, truthfully, we do not need to render on each request, but doing something smarter at build time would help us. So let’s do (4) and then we can do (3) if it turns out to be necessary.

(4) is easier because we don’t need to do any snabbdom-to-html stuff; just get the data and add it to the `index.html` somehow. Or just write a file that gets loaded in `index.html` — this will get bundled when we build the site.

What might be interesting is if we were to fire the initial command during the build … this causes the right data to get fetched … but then what do we do to persist the initial state somehow? Maybe in a script we could ask the display for its current state somehow? Seems too complicated …

If we get the data at build time, then there’s really no special reason why it needs to be in CosmosDB at all … and I mean remember that the data is not currently in Cosmos …

So we could just inline the data. But the problem now becomes testing. How do we control the learning areas during the test? Especially because we’ve been treating the display (during the test) as simply a static asset which we control by controlling the api side. Can we somehow control the UI as well when it is loaded during the test? 

Yes … Vite has a ‘middleware’ mode where we are in charge of serving the `index.html` so we could inject things that way. Basically simulating SSR. And ultimately we do want to server side render the engagement plans etc once someone has logged in. So maybe we should just switch to that now.

But then we need to figure out how to actually deploy that … with Azure, I think we’d need a blob for the static assets and a function for the `index.html`. 

ACTUALLY … we found a better way to do this.

With static web apps, you can specify rewrite rules for particular routes in the `staticwebapp.config.json` file. In there, we set a rewrite rule for `/index.html` (which matches, `/` also) that rewrites the request to go to `/api/root`, where we have a function app deployed that actually generates and returns the `index.html`. And it works!!

We do pay the cold start cost, but when the request returns, it immediately has all the data we need and renders the page as expected.

This seems like a model that could be followed to create multi-page apps too, if necessary.

### Authentication

Azure static web apps has support for authentication built in. The trick is going to be running this locally and during the tests.

The `swa` cli can standup a mock auth server, which is good. But if we use that, then we need to run everything through that when we are running locally or during tests.

The `swa` cli needs the azure function core tools to work. Unfortunately, these aren’t built yet for arm64 I guess. Nevertheless, I downloaded and installed dotnet for arm mac and then cloned the function core tools repo and built it myself. It seems to run fine. However, there’s some problem with the `swa` cli where it doesn’t recognize the `func` command in the path and so I need to actually just start it separately and give the `swa` command a url to the running functions server.

But then to get things going I also need to have fake cosmos db running and pass the url to that in via an environment variable. AND I have to build everything before hand so the function simulator can actually find the JS code to run. For that reason it might be difficult to use during a test I think.

What I’d really like to do is just have the SWA emulator handle the auth requests and me do everything else. Now, it is possible to do that, but it’s picking up the staticwebapp.config.js file so it’s trying to use the root function app to render the index html etc …

Maybe we try providing a different static web app config for the tests? That seems to work! And if we set the `--api-host` to our local server, then it just forwards requests there and it works since we use the same path in our express server as the function service would. Now if we could run `swa` programmatically, that would be awesome …

Actually, once we set the api host to the same url as our local server, then it just works since the paths are the same. It can actually use the standard static web app config file to redirect index requests to the root function.

But we have a problem here about how to figure out the logged on user from the request. Where should that logic live? Part of our app is dependent on the deployment scenario (the code for azure functions etc). We want to keep that as light as possible. But in this case, figuring out the logged in user is part of that.

We decided to treat this as a deployment-specific value. It’s up to the deployment to get this value and pass it into the `initialState` function.

We refactored things so that the local server and the root api function share as much code as possible — so that when we run the tests we’re actually exercising nearly the same code that will be running in production.

And now authentication works!

One other thing on the tests. We discovered that `pageText` is kind of dangerous since it can return content from script tags which could lead to false positives. For example, our test searched for the user’s identifier on the page after they logged in, but what it was finding was that value in the script tag text that gets written by the root function to set the initial state. We really want ‘text that’s on the page and visible to users’ somehow. Actually, we will continue the pattern of using domain specific selectors to help us narrow assertions appropriately.

I’m curious to do some investigation here … 

1. What actual claims and user details are we getting from the github authentication? It says that we have access to email, so can we display that instead?
2. We know that the emulator passes the special header that contains the base64 encoded principal to the backstage functions. We know the emulator also passes an `Authorization` header with a bearer token. But in the emultaor these are the same value. Curious to know if in production we get a different bearer token (one that is a JWT or signed or whatever) that could be used in other cases or if we still get just the base64 encoded principal.

It turns out that there is *always* an Authorization header, even if the user has not yet authenticated. I suppose this secures the function so it can only be called by the frontend. This authorization header is different from the `x-ms-client-principal` header, but does not change whether the user is logged in or not. (so it’s not the *user’s* token, I think)

ACTUALLY, the tokens *are* different … they just start out the same. When you are authenticated, the token contains a `prn` claim with the base64 encoded client principal object. And actually the tokens are very different.

Without auth:

```
{
  "nbf": 1650111305,
  "exp": 1650111605,
  "iat": 1650111305,
  "iss": "https://15f0e99b-d9ac-451d-a813-98b668a3eea1.scm.azurewebsites.net",
  "aud": "https://15f0e99b-d9ac-451d-a813-98b668a3eea1.azurewebsites.net/azurefunctions"
}
```

With Auth:

```
{
  "prn": "eyJpZGVudGl0eVByb3ZpZGVyIjoiZ2l0aHViIiwidXNlcklkIjoiZjQ2YjYyYmJkYzU2NDhjMDgzMGNhM2FhYzQ3ZDQ1ZDAiLCJ1c2VyRGV0YWlscyI6ImJyaWFuLXdhdGtpbnMiLCJ1c2VyUm9sZXMiOlsiYW5vbnltb3VzIiwiYXV0aGVudGljYXRlZCJdfQ==",
  "sub": "f46b62bbdc5648c0830ca3aac47d45d0",
  "iss": "https://mango-rock-0e703a30f.1.azurestaticapps.net/.auth",
  "aud": "https://15f0e99b-d9ac-451d-a813-98b668a3eea1.azurewebsites.net",
  "nbf": 1650111314,
  "exp": 1650111614,
  "iat": 1650111314
}
```

Also, the client principal json object does not contain the user email. Only the user name, ie your github username for github. Maybe it’s possible provide another claim somehow?

One extra thing we did, which is recommended in the docs, is to create a route rewrite rule for `/login` that rewrites to whatever login provider path. This allows us to control the login provider in the static web apps config file, which is nice (instead of in the view). And makes our code ultimately more portable, if we decide to change deployment strategies.

Also tried using azure active directory as the auth provider. It works the same as github … doesn’t provide extra info, like name.

### Paying attention to jumps in test suite run time

Our tests are a bit slower now … jumped from ~1.3 seconds to ~5.3 seconds. This is due to starting and stopping the SWA server on every behavior. We need the SWA server because it helps us fake authentication during the test.

To address this, we made the SWA server stay running throughout the test suite. This cuts the time of a run from 5.3s to 3s, which seems worthwhile and will pay off more as we add more behaviors.

We could also consider running with the SWA server ONLY when we need to log in. But I imagine most of the remaining behaviors will require us to log in so maybe not so worth it.

Also, we noticed that we were starting and stopping the vite dev server on every behavior (whenever we started and stopped the test server) so we refactored that so it starts once and runs throughout the test suite run. This took the run time from 3s to 2.6s. Again, this seems worth it and will pay off more as we add more behaviors.

### Adding another feature

Now I’d like only people who have authenticated to be able to select an engagement plan. First we add a test that shows that you do not see the engagement indicator when you are not logged in. And then we add to our existing test (where we select engagement plans) a step to login. We can get these passing by giving the user object to the learning plans view and having it check for things. But we have a few problems:

1. The signature for the learning plans view is getting big … this is starting to feel like ‘prop drilling’ in react I guess. Maybe we should switch to a ViewModel style approach? or otherwise store data in the model in a better way? The key though here is that we have the test passing. We could do a commit now and experiment with different refactorings.
2. We have some weird things now. We have some data in the DB about engagement plans. So the title of the engagement level will show up now when we deploy, even though we aren’t logged in. But we can’t really write a test for this because now that we ship this feature, it’s not possible that this could happen. I suppose that this will actually get fixed when we do the feature to only show the logged in person’s engagement plans — right now we still just show THE engagement plan. But in the meantime, our app in prod will be in a bad state. Perhaps we should drop the data to fix it.

Interestingly, by addressing (1), we were able to address (2) as well — even though we don’t have a test for it.

We changed the /state/ of our app to better match how our app works. Now, there are two main states — informative and personalized. The informative state simply displays a list of learning areas (and a login button). The personalized state is only shown once a person has authenticated. Then, they see the engagement levels for each learning area and have the ability to set their engagement level. We still aren’t associating this with an individual and there’s still more work to do on the engagement level control. But it accomplishes what we want — you only see stuff about engagement levels once you’ve logged in.

We still couldn’t write a test for (2) … but our test suite implies that someone can see engagement levels only once they’ve logged in. And now our app does that, even though the data isn’t yet associated with a particular person.

The next step is either to work on the control for selecting engagement levels or to associate engagement levels with an individual.

### Selecting multiple engagement levels

It makes sense that we might want to have multiple engagement levels … or does it? I feel like engagement levels are not exclusive but additive. So for TDD I, personally, and learning, doing, /and/ sharing. It doesn’t really make sense that you could be ‘sharing’ but not ‘learning’ or ‘doing’ though so that seems to be a business rule, namely, that you have to select these in order: learning -> doing -> sharing.

But once we have multiple engagement levels, it seems like we want to change how we’re storing data. Right now, we store a different document for each engagement type. But maybe it’s better if the document we store has a learning plan id, and a list of engagement levels. This means that if we were to take away an engagement type, we would just update this document. But if there were no engagement types, then we would delete the document, I guess, or just update it to include an empty list.

We’d also want to consider which types of engagement you could deselect … according to the rule, you can only deselect the highest level. 

Finally, we probably want to debounce these actions and save after a slight delay so we don’t overwhelm the database or get the UI confused. Alternatively, we have a save button somewhere, but I’d rather avoid that if possible. And if we have a save button, maybe it’s for the entire list of areas. So, once you make a change, you see something that says Save or Cancel and you can continue to make changes. If you cancel, then all the changes disappear. If you save then they all get saved.

But if the order is really additive and fixed. Maybe what we need is just something like ‘Increase Engagement’ and ‘Decrease Engagement’ buttons and you can click ‘increase’ multiple times or ‘decrease’ multiple times.

Do we really want to ‘decrease’ engagement though? Maybe it’s just a toggle like on tracker … there’s one button and I click to go through learning, learning + doing, learning + doing + sharing, not engaged. So if I mess up, I could click through again to get to the right level I guess. Maybe the button is like: ‘I want to learn it!’, ‘Let’s do it!’, ‘I want to share what I’ve learned with others!’, ‘I’m done for now!’

If so, we could still keep the data model the same, and it’s just that when we click ‘I’m done for now’ it deletes all the engagement plan documents for this user with this learning area.

### Problems with Azure Cosmos DB

Our tests pass locally but the items were actually not getting deleted from the database I think. The problem was with the partition key. We were using `id` value as the partition key but then you can’t actually do a batch update since all the items in a batch update must have the same partition key. (Documentation is non-existent on this but it seems to make sense)

So, I think I need to change the partition key to be the userid. This makes sense so that each user’s data will be stored together on one partition.

For now we just hardcode some fake userid but it works and now we can batch delete the engagement plans.

This actually moves us a bit of the way toward storing data on a per-user basis, so that’s good. 

We still are querying the engagement plans for the learning area before batch deleting them. We should probably instead pass the engagement plan ids around so that we can just skip the query and do the batch delete.

### Storing engagement plans per user

But now we have a kind of issue. When we store engagement plans, we need to have the current userid to associate with them. But as it stands now, our view knows about the message to send … basically a backstage message that says write this particular engagement plan. It doesn’t seem ideal that the view knows about backstage messages. And now, we will need to pass the userid through all the various view functions so that it too can be included in this message … since we will need the userid backstage when we write the engagement plans.

Probably what we want to have is some notion of an effect. So the view can send an Effect message that indicates the engagement plan to write, and then the effect manager converts that to a backstage message and includes the userid from the state.

This would allow us to keep the view focused on publishing messages with the minimum data that it knows about and agnostic of other details of the implementation — like that it needs to be a backstage message — while we can make use of the state as a whole in building the backstage message. 

But it kind of complicates things a bit due to the way redux works. We now have /another/ middleware, one for handling effect messages and then we have the main reducer that updates the state.  

Actually, what might be better is this … what we really need here is the userid. But the backstage actually knows this already; we just need to parse it from the header. So we could change the backstage handler function so that it has a user object that gets passed in along with the message. That would be all that we need for this case. Still we are left with the view knowing which messages are backstage messages but we could avoid having an extra handler function for now. (although probably at some point we may still want the ability to pull things from the state without passing them through the view functions when constructing a message to send, but we’ll see …)

### Design clean-up

We have a massive amount of design debt that we need to clean up.

We’ve introduced a better card view and we’re getting it to organize in a flex box in columns. Maybe we should actually have columns like ‘Practices’, ‘Team’, ‘Values’ or something like that — because there’s going to be lots of these and then they could scroll in their column. And when we click maybe it overlays like on trello? 

I think maybe actually when I click it should go to a new page where it displays all the information about this item. That makes it so that we can share the links more easily too. And will force us to think about navigation in our app too …

### Organizing learning areas into groups

The design clean up lead us to pick up another story around organizing learning areas into groups or categories. We did this so we could display the learning areas in three columns on the screen.

We were able to implement this fairly easily, but now we have some kind of ugly code to produce the html … it kind of repeats but has several different variables over the repitition. 

We could create a function with those four inputs. But maybe we need to look for an abstraction here to help us understand the meaning of the inputs …

So I did some refactoring and actually introduced a `LearningAreaCategory` module with the view functions related to it. This allowed us to pull that out of the `LearningArea` module and make the view code a little easier to understand and follow, hopefully.

### Accessing learning areas via a link

I want to actually refactor the app so that when you click a learning area it goes to a new page to show the content, and that page can be accessed directly via a link. The value, I think, here for the app is that it will be easy to share links to particular learning areas. It also means that we don’t need to load the content (or, eventually, notes) until we view a particular learning area.

I wrote a test to access a particular page of the app that should show a specific learning area, and it fails. So now we need to make it pass. My hope is that this will show how resilient our tests are to major refactors, since we already have some tests for viewing content of learning areas that should not need to change — since you will still be able to access the learning area content page by clicking on a learning area from the main page.

I’d also eventually like to experiment here with prefetching these content pages somehow, to see if it’s possible to speed the load time, but we’ll see.

In order to have another page rendered with data from the server, we need to create another backstage I think. And really I need to make it easy to add new pages. 

For a new page, I need:
- index.html (ie just some html template that will get served)
- some javascript that references the display, mounts it, and starts redux
- a function to render the html with the initial state
- a function to handle backstage messages. 

For /this particular/ new page, I really just need the first three since there won’t be any backstage messages I need to handle until we can add notes.

But actually, if I open the learning area on a new page, I’m going to move the button to increase the engagement level too. So really its the /main/ page that will no longer need a backstage function. And in any case, we could keep backstage ambivalent about the display page that is calling it. It’s still just a backstage message that gets processed. 

ACTUALLY, though, we still need each page to have a backstage function since the main page needs a different initial state than the engage page. It’s just that the main page won’t have to process any messages (at this time).

We probably want to take this opportunity to move the app-specific files in `display` to the `src` directory … and make `display` more like a library.

And when I build, I’m going to need vite to produce two html pages …

So I wrote a test where I visit the link I want specifically and I see some text on the page. Just did the minimal markup to make it pass. And I switched the old markup on the main page to link to the new content page when a learning area is clicked (rather than firing a display event). Then I ran all the tests. I see that the tests around engagement levels and markdown content fail, which is expected. 

So I focus the text on markdown content and start copying over the code from the main page markup to the content page markup. Now that test passes. And I can start to comment out code in the main page around what happens when a learning area is ‘selected’ since that’s now just a concern of the content page.

Next is to move over the personalized state. The first test around that is failing at first because it cannot find the button to increase engagement. So I copied over the code for that. Now I need to modify the test because the behavior has changed slightly … Since the increase engagement button is on the content page now, I need to go back to the learning areas page to select another learning area. The test goes through and selects different engagement levels for multiple areas to cover the different cases. It seems that I’ll need to also observe that when I go back to a content page, the right engagement levels are selected — but I also still need to see this on the main page as well. So, I added a step to return to the learning areas page, but now we’re failing because the button text needs to change as the engagement level is increased. This means we actually need to start handling display messages for the engage page …

At each point here I’m letting the test failure guide me in this refactor.

So we copied over the code to handle the backstage message to write an engagement level and the display message that an engagement level was persisted. But now we have a problem. When we go back to the main page with all the learning areas, we don’t see the updates to the engagement levels. If we /reload the page/ then we see the updates because they are fetched as part of rendering the html. But just going back does not trigger that, and otherwise there’s no communication between the main page and the content page because they are just separate html pages. 

In short, there’s no /session/ that’s maintained between these pages. In a single-page-app we could have some kind of shared state that both pages can access. But in a multi-page app we don’t have that particular option. We kind of have to go back to the old school strategies: a session maintained on the server, a cookie of some sort, maybe. But there is a kind of new thing we could take advantage of … local storage and/or service workers … 

So if we had a cache of engagement levels in local storage we could check that first if we are just navigating back to the page … And now maybe redux starts to become nice because we could just serialize the app state to local storage maybe or some part of it? Maybe `sessionStorage` actually makes sense because we don’t need to persist it all the time. And if someone uses a different browser then when they come back there’s no worry about invalidating the cache etc. Looks like we would need to leverage the `pageShow` and `pageHide` events too to load and save the state. 

Here’s one thing I could do:
- Subscribe to redux and write the learning area to the session store
- In the main display, on pageShow event, get a learning area and dispatch a message, then delete the session store
- process the message to update the engagement levels for that learning area.

Or:

- The main display subscribes to the redux store and writes everything to session storage on changes
- the content display also subscribes to the store and updates the session storage on each change to the learning area.

This seems more complicated and the content needs to know about the shape of the state stored in the session storage

Or:

Could somehow write all the messages to the session storage and when a pageShow event happens, read all those events and dispatch them so the main store gets updated if it cares about them.

But there will probably be several messages the main page doesn’t care about … 

I guess maybe we could have a filter that determines which messages to persist. This would have to be middleware anyway

OR:

I could flip things so that the display has a reference to a `Program` or something and I could call functions on it like `subscribeToStore` or `dispatchMessage` where I could do the things I’m talking about. This does expose a bit more than we would probably like though.

Definitely seems like a `subscribe` function could be helpful … this would kind of be like the `view` in that it would be called with the latest state each time it changes. This function could do side effects like store some part of the state in local storage.

But elm has a function for which subscriptions you’d like to register. These are like message-generating functions that you can either turn on or off depending on the model. That’s kind of what we need for the main page, to dispatch a message on a particular event, namely `pageShow`. We could do the same thing on the content page I guess, with the `pageHide` method, although it wouldn’t need to dispatch a message but perform the side effect of storing some of the state in local storage. 

OR:

I could have the main page just request the engagement levels again on every `pageShow` event somehow. That would not require local storage but there might be a more noticeable gap between loading the page and showing the updated engagement levels.

OR: 

Maybe we just need to have one `AppState` across both pages and it’s always stored also in session storage. Then on the content page, we would just need to update both the full learning area object, which is displayed on the content page, AND the relevent item in the list of learning areas … so when the main page comes back again, it has the right state to draw. (Or maybe rearrange the model so it makes sense, like maybe by storing engagement levels separately from the learning area, or just loading content when we go to the content page). 

The only problem with this is the fact that when we go directly to a learning area content page, we need to have initial state for that page, and we wouldn’t want to fetch initial state for the main page too right? But if you go there, then you’d have to click a link or something to get to the main list, which would then load that page along with all the latest data it needs. The trick is that somehow we need to reconcile the initial state with the app state stored in the session storage. 

1. User goes to main page; initial state is loaded.
2. User clicks learning area and goes to content page, which loads with data.
3. User adds engagement level, which is persisted in the store and saved to local storage.
4. User goes back to main page, which uses the state in local storage to refresh.

Note that when I go `back` to the main page, it does not execute any javascript, namely, it does not call the `view` function again.

But note that somehow Chrome does trigger the view to run again. It must just execute the javascript again just as when the page loaded. Safari does not seem to do that.

Note that the `pageShow` event only seems to work with the window element in Safari, even though MDN says it also works on the document.body element. Actually, it does but only if you set it using `document.body.onpageshow = ...`

We can use the `persisted` property of the pageShow event to tell if the page is loaded from the back-forward-cache or if it’s fresh.

UGH … this is not working great though:
- if I try to go back from the content page, the state at that point does not have the list of learning areas and so if I refresh the state, it will overwrite the learning areas fetched on the main page.
- if I record the messages and then play them back when the main page is shown, then I get some things updated, but if I go forward and back again, I lose those changes. 
- if I keep a running list of messages and then play them all back each time, I get the test to pass. But now I have a running list of broadcast messages that will continue to grow …  and in safari, this means we start to get duplicates of things because it is keeping the JS state in memory apparently. In Chrome it works though, because Chrome apparently re-runs the javascript. Basically it seems like safari is different here, so for safari we would need to reset the state to the initial state when encounter a pageshow event (we could do this with every browser I guess)

But this all seems kind of not so great ...

One thing we could consider is having the state for the content page be a subset of the state for the main page. And then it's just that particular state that would be persisted in the session store and merged back with the main state. So for the content page, this would just be the selected learning area and the personalized state with the list of engagement areas. It's still weird since the content page doesn't really need to know /all/ the engagement plans, just the plan for this particular area. But it could be a nicer way to define the state that needs to be persisted. But even here, there are some pieces of state that we don't really need to persist ... like the content of the selected learning area.

We could have some notion of 'page state' and 'app state'. But then how would
that work with the reducer? One message could potentially update both I guess. 

We could have a special attribute for the shared state. Or we could have
separate reducers for page state and global state.

Maybe actually I need a service worker? But this isn't going to work well I bet
since we just have one route on the server side that we are using. But what
would we actually cache? It's the whole request for the page I guess? But it
would be very difficult to update the state in that cached html I think, based
on what has happened on other pages, right? Maybe we should just figure out how
to reload the page when the back button is used?

Session storage works but is tricky to generalize I guess. 

Consider this though:

1. Load the main page -- this gets us the list of learning areas and the app
state (informative or personalized) which can include the user and the list of
engagement plans.
2. (You can't do anything on this page to change the state; it's purely
informative)
3. Click to navigate to a learning area content page. On page hide we save the
current state in the session store.
4. The page is loaded from the server with the full learning area (let's
say) including content. We take the server content and then apply over top of it
the state from the session to get the initial state. So now we have the
list of learning areas and the engagement plans and the user, supposing this is
personalized.
5. A user clicks to increase engagement level.
6. This is persisted on the server, and we update the list of engagement plans
to reflect it on the UI. 
7. The user presses the back button and on page hide we save the current state
in the session store.
8. [In Safari] The pageShow event can trigger a refresh of the state. We use the
current state in the session store. This state contains everything we got
originally from the main page.
9. [In Chrome] the javascript is rerun, which I think triggers the initial state
and we apply the state from the session on top. In addition, the pageShow
event replaces the state with that which is stored in the session
store.


Consider the other way:

1. Load the content page -- this gets us a particular learning area, with
content. So we don't have enough to render the page here. We need also the app
state, which includes the user and the list of engagement plans.
2. Suppose we click a link to load the main page. And on pageHide we save the
state in the session.
3. Now when we load the main page, the initialState function will override the
server state with what's in the session. But this will mean that there's an
empty list of learning areas at least ...

One alternative is that each page loads the entirety of the initial state, not
just the part that it needs. This means that 'initial state' is really the
initial state that any page in the app might need in order to render. Then we
could do what we are describing above. 


But maybe actually what we could do is just create some kind of event handler
function for `pageHide` events. The handler gets the current state and a
dispatcher. It dispatches a `PersistMessage` which contains some slice of the
state. Then we have some middleware that will store that message in the session
storage. On `pageShow` we look in the session storage to see if there's a
`PersistMessage` and if so we pass it on. The root reducer will just take the
slice of state in the message and use it to patch the state. This would probably
work. But what about going back several pages? In other words, should we delete
the stored message or keep it around and add new messages to a list? 

Is this a bad road to go down? What if there are many different pages in our
app? But you can only go back in a straight line, I guess. And any time we load
a page from the server then we should assume that we have all the info we need.
So, as part of initially loading the page, we should probably remove any
`PersistMessages` from session storage. Or, on a `pageShow` event that has not
been persisted.

This kind of works. But Chrome is a bit finicky. It basically seems to run the
`pageHide` event handler in the background while the next page is loading. So,
our event handler does update the session storage, but the main page doesn't
have the new content because apparently `pageShow` is called before the write to
the session storage completes. We can fix this by using a `beforeUnload` event
but this is not recommended for various reasons.

So another option is to update the session storage on changes to the state
rather than right before the page transitions. But this could have performance
implications in certain situations since we have to do some JSON.stringify
etc. This is probably the better way to do it though, I suppose.

So we got all the tests to pass. We created a `SessionMessage` that can be used
to store a slice of the app model. This gets merged into the model on `pageShow`
events.

I think though that it's probably better to have this not be a framework
concern. So long as we expose `getState` and `dispatch` on the display object, I
think we could do this all as part of the application code. Moreover, we might
not want to persist part of the state anyway, but some subset of it. In our
case, when we have notes, we'll want to display the number of notes for each
learning area on the main page, but we don't care about the notes themselves.

And if we were to be more specific, we could update the state better -- like
send a message to the reducer on pageShow with *some* data to use. But not have
to worry about maintaining all the state *another* page needs.

But to do this, we need to switch things so that the app is an object we create
rather than a set of functions we export ... and that would require moving the
html pages into the src code, which is actually probably a really good thing.

But do we need to subscribe to state changes too? It would be kind of
interesting if we just had another function that *also* received any messages
and could just do effects? For example, in our case, we want to record the
engagement levels for this learning area in the session store. This is also part
of the model. And if we change things up then we will *simply* need to store the
list of engagement levels for this particular learning area.

We want to update the session state when certain things change, but maybe we
shouldn't handle the message in two places?

I guess I'm really just worried about the subscribe method getting called tons
of times. But really it's fine though ... we're probably not going to do
anything that's updating the state frequently.

So now I've abstracted the display logic but I've got a problem :) -- before the
display was actually different for each page. The main page did not add the
session middleware but the engage page did. So now the main page is getting the
session message on page show but then the middleware catches it and just stores
it in the session again. What we need to do is store a *message* (instead of a
slice of state) and dispatch *that* on page show.

Big problem, though. We can't seem to test this in playwright ... it just does
not cache the page in the way that the browser does. Not sure why. And in any
case, I'm sure there are some times when the browser would not cache the page.
So maybe what we need to do is just find a way to disable caching for this page.
So that when we go back, the page will be refreshed no matter what.

I cleaned up the code and checked it in ... but I've still got lots of problems,
primarily because the serverless function code is not actually tested ... need
to find some way to get that under test or otherwise make it so that it doesn't
need to change.

But note that now that I'm using `https`, the cache-control header seems to be
respected by safari ... so now going back after making a change works fine and
there's no jump in the page when it renders!! It seems to work really nicely,
and takes less than 200ms to load the page.


### Abstract the serverless functions

We have a problem where the `local` server and the serverless functions often
get out of sync. We need to find a better abstraction so that we're testing, as
much as possible, the same code that we're running. As it stands now, we just
don't know that things will work even if all the tests pass.

We've already created a kind of abstraction, namely the `Backstage` abstraction.
This is pretty good, but we're finding that there are *Azure* specific things
that we need to do also. For example, Azure stores the user details in a certain
specific header. And when we're deployed to Azure, and a request to the engage
page is rewritten to go to the engage function, the original url is passed in an
Azure-specific header; and we need that original url to get the path parameter
that indicates which learning area to display.

Both the `update` function and the `initialState` function care about headers,
since that is how they get the user details. And the `initialState` function may
care about the path (as in the case of the engage function). I'd like to keep
the Azure specific things in the serverless function code, I think.

The best solution would be to run the `swa` cli during the tests to server up
the same code that gets run as a function. However, this doesn't seem feasible
since we have to compile the ts code into javascript and for our tests we want
to be able to inject various things to control execution. Instead, maybe we need
to write an adapter around the http trigger function somehow.

So, if in the serverless function code, we expose a function that takes a
Backstage object and returns an AzureFunction, maybe we can just change local to
adapt an express request/response to an AzureFunction. We're not using anything
too special about the Azure function context, and the request object is similar
to that of express. The trick would just be to take `context.res` and translate
that to the output express uses to send the response back.

The goal ultimately is to be able to deploy everything to another platform, like
netlify or whatever so we do want to continue to distinguish between
platform-specific code and app code.

We did it ... basically the local server just uses the adapters to create an
exposed AzureFunction and then there's an AzureFunctionAdapter that adapts this
to run in an express route with express request and response objects. So now we
should not have to change the local server as much, and we are exercising the
very same code that will run in production.


### Cleaning things up a bit

The `Backstage` abstraction had functions for both getting the initial state and
performing backstage actions. But these are two totally separate cases, and some
pages might not even need to do both. So, we separated these out into two
distinct things. Now, a page can expose a function to create a
`BackstageRenderer` -- which right now just has a function to get the initial
state. And a separate function to create a `Backstage` which handles messages
from the display.

We could probably improve the `BackstageRenderer` abstraction. Right now there's
some common code in the API that handles fetching the template and replacing
some text with the stringified initial state. This feels like it is not Azure
specific and so could be abstracted away somehow. 

The other thing we did was to introduce another source directory called
`overview`. So now, there is `engage` and `overview` as the main pages of the
app. And because we run requests for pages through the backstage renderers, we
effectively have decoupled the pages of our app from how they appear in the UI,
ie what order or at what url. This is interesting but might be kind of weird as
we add more links among sections of the app.

But it raises an interesting idea ... to what extent should your source code
care about the navigation and structure of the app. Maybe that too is more a
function of how the app is deployed. The structure of your source code perhaps
should allow you to understand what the application does and just make it easy
to find things. The important thing is to have certain capabilities available.
How those capabilities are wired together could/should be externalized. (maybe
this is kind of what azure functions does with the `function.json` file)

But there are a few problems to consider with this:

1. How do we handle `shared` things. For example, each page needs to display the
logged in user, and should do so in a similar way. Is that a different
capability? Or do we have a folder of shared things? 
2. What about adapters and other parts of the system that touch the outside
world? Some of these things may be tied to the deployment environment (we
probably would not use CosmosDB if we were not deploying to Azure, for example).
So maybe they should be outside the src directory? But then should there be a
top level directory called adapters?

I think for (2) we probably do want a top-level directory. And maybe we call
that directory `azure` -- ie call it after the deployment environment. It would
contain all the serverless function stuff and deployment-environment-specific
adapters.

For (1) I think the idea is to have a shared function probably. The alternative
would probably be to have something like a frame that capabilities fit into. And
so the page html itself might get more complicated. Or something like that ...
displaying the user/logging out/logging in seems like a very simple 'application
capability'. Why can't you have multiple capabilities on a page? (This is what
remix does I think)

So in that case there would be another capability (another folder) called
'auth' or something.

But how would you put these capabilities together on the page, and how would
they get the necessary initial state etc? 

There's another kind of shared thing too ... constants like
`LearningAreaCategory` and `EngagementLevel`. These are basically strings or
identifiers that both capabilities need. We could duplicate them but then it
seems like that would make things harder to change since they need to stay in
sync.

I have less problem duplicating data types. So I think there's a difference here
between values and shapes ... we have `LearningArea` repeated. But in one case
it contains a `content` attribute and in the other it does not.

One thing to consider is that the type may not matter. In some cases, maybe we
just treat the engagement level like a string. In other cases, maybe it's
important to make it typed, if we have to do stuff with it. 

So, for now, only in the engage capability will I have typed values for
`EngagementLevel`; for the overview capability this will still be its own type
but it will just be a type alias for a `string`. And, similarly, for the
`LearningAreaCategory` type ... this will only be a typed value for the overview
capability; for engage it will be a string.

So, on one view we are duplicating code ... but actually we just have different
types with the same names. This is similar to what we do with the shape of data.
There are two types called `LearningArea` but they actually have different
properties depending on what the capability they are associated with actually
needs. 


### Redirecting on bad requests

Azure static web apps has the ability to do 'response overrides' -- so if a 404
is found then you can redirect to another page. We'd like to make it so that if
someone tries to load a learning area page that doesn't exist, they get
redirected back to the main page.

In order to do this, we need to allow the `getInitialState` function to return a
more complicated message ... it needs to specify whether a template should be
rendered or something else should happen (like return not found or redirect or
something).

We first tried returning a NotFound message and then having Azure Static Web
Apps notice this and do a redirect. It didn't work. With the emulator, none of
the response overrides stuff worked -- this could be because we're using a local
server rather than having the emulator serve the files. We deployed on a branch
and random 404s would get redirected as expected, but the 404 from the function
was not redirected, perhaps because it's a function and handled separately.

In any case, we decided to make it so that the function returns an actual `301`
response, thus accomplishing the redirect itself. We left the config in place to
redirect other 404s to the main page, but there's not a good way to test this
locally I think. 

One interesting thing is that we utilized the Azure static web apps ability to
deploy pull requests at a preview url. We added to our github workflow the
ability to trigger on pull requests and these get automatically pushed to a
preview environment. Pretty cool! The clean up didn't appear to work though, so
I think we have to go in and manually delete the preview environments when we're
done. There does seem to be a github action job for this, but it didn't work
when we tried it for some reason. We probably won't use this very often so it's
fine to delete manually for now.


### Rendering markdown on the server

Currently we rendered markdown into html in the UI. But this means we had to
include all the libraries we were using to do this into the frontend vendor
bundle. I noticed that this seemed to be pretty large. 

It was actually quiet easy to switch the markdown rendering to the backstage.
Since we are working in typescript across the entire project, and since the
library we are using is isomorphic we kind of just did it. Now the learning area
content is an HTML string rather than a markdown string and we just let snabbdom
display that directly.

This decreased the frontend vendor bundle size by over 50%! And we were able to
do it in literally minutes and we could run our test suite to ensure everything
still works. And we didn't need to make a single change to our test suite at
all!

Now, I'm not sure this is the ideal end state. It's not great, I think, that
we're treating the html content as a property of our model. We don't really need
to hold it in memory because we aren't going to do anything with it -- just
render it. This was the simplest thing to do that would get things working.

BUT, honestly we had a few problems with this ... First of all, the content
disappeared after one view because our implementation was actually modifying the
static learning area content. So we wrapped that in a function to make it
immutable. Then we remembered that we needed to explicitly externalize any
dependencies when building the server side so they don't get bundled with the
code. When we did this, the engage bundle size went from over 300k to around 9k.

Ultimately though it would be good to render the html on the server for the
content and just have it be part of the web page html somehow. That way we are
really doing server-side rendering of html content and not just server-side
fetching of data that's necessary to render the html on the client. But that's a
bigger change, and we haven't really seen that it's all that necessary.

If we were to do that, there's a few options we could pursue. But one thing that
might be interesting is to make the areas of the page governed by Snabbdom and
our app framework be more targeted. So just the part of the engage page that
deal with incrementing the learning area would be governed by our app. And then
maybe also the part that displays the user would be another thing. And then any
static html content could just flow around this somehow. I have no idea how this
might be implemented but something like that might help us handle server-side
rendering for real and take a more decomposable approach to the UI. 


### Keeping an eye on test speed

The tests are getting slower ... up to almost 7 seconds on my machine, from less
than 6 seconds a few stories ago. We looked over the test suite and probably the
engage test is the one that takes the longest since it is opening lots of
windows and logging in multiple times. We changed this so that it only logs in
again if a different user needs to log in. Otherwise, it simply reloads the
page, which is sufficient for showing that things have been persisted. This
shaved off over a second from the run time.

In general, it seems like opening a browser window is what slows the test down
the most. Nevertheless, it's important to do this so each example is isolated
from the others. We could try to optimize further but it might just end up
making the test suite harder to reason about -- like, is this an example in a
new context or is it sharing a context? etc


### Cleaning up the styles

We're using tailwind and that can definitely clutter up the view code a lot. We
also start to get a lot of repitition everywhere, so we tried to dry this up a
bit. We abstracted in two ways:

1. Pull out collections of styles into a `Style` module
2. Pull out reusable views into a `ViewElements` module

This allows our view code to reuse code at different levels. Sometimes, we might
want to have the same style, but it doesn't make sense to make a shared view
element yet.

Probably the best thing is creating some code for organizing the colors and the
text colors. One thing with Tailwind, though, you should not dynamically
construct the class names, since Tailwind actually looks through your code to
determine the minimum stylesheet to produce. So you have to create functions
that use the full tailwind class name always.

Our rule for abstracting collections of styles or view elements is to keep the
styling within the bounds of the element and leave layout info (margins, flex,
etc) to the elements that use these. So the view code pretty much only has
styling information about relative positioning -- border, colors, typography,
padding, etc is all abstracted away as much as possible.

What we discovered in doing this is that we sometimes had duplicated classes
(even on a single element). We sometimes had slight inconsistencies (different
border colors that were hard to distinguish). And we sometimes had different
implementations that resulted in the same thing. So this exercise allowed us to
get more consistent across the board with the approach to styling, colors, etc.


### Logging in on the engage page

When someone clicks a learning area, they are taken to a separate page with the
full content etc. That page did not have the login button on it yet. So our next
feature is to allow people to log in from a learning area engagement page. This
makes sense because we imagine someone might link to a particular page or share
that link, and then they could log in there.

The first pass at this is straightforward and we modified some tests -- just add
the login button. But what happens is that upon login one is directed back to
the main page of the app. Instead, we want to redirect to the same page you
logged in from.

It turns out that Azure static web apps can do this, by adding a query param to
the login link. However, we were abstracting this particular link away via a
rewrite rule in the static web apps config. This is nice, because then we keep
this Azure specific detail (how to construct the login link) out of our source
code, making it potentially easier to deploy in another environment.

The first pass to implement this is just to allow the engage page to construct
the link based on what learning area is displayed.

Interestingly enough, we had thought of the login button / user button as a kind
of separate capability. But now, what we're seeing, is that the overview page
needs a certain kind of login button and the engage page needs a slightly
different one. They both should look the same, however, but it feels weird to
somehow pass in a redirect link in one case but not in the other. So we have two
login buttons right now. They share a common view element called a `linkBox` so
the only difference is really in the particular login url (the `href`) that we
pass to it.

However, we now have Azure specific details in our source code. How can we
abstract the login link in some way that insulates our app from this detail and
works to make it more portable?

It would be great if Azure static web apps was able to rewrite the login url in
a way that took into account query params we pass, so that we could pass our own
redirect param and it could rewrite it into the format that static web apps
wants. However, the static web apps rewrite rule does not seem to be that
sophisticated.

Maybe we need some kind of frontend adapter? We have these in the `azure`
directory for the backstage ... is it possible to do this for the frontstage?
Or, it could be something that we provide from the backstage via the server side
render of the HTML?

Another thing we could do is just write a serverless function that redirects to
the azure login link. So in that way we could maintain the `/login` endpoint
and just add our own query param for redirects, but we would rewrite to
something like `/api/login` which would in turn redirect to the azure login
link. This is probably the best approach.

We did this but ran into some problems when deploying, mainly because we tried
to mimic the same setup from before where we specify the particular auth
provider in the static web apps config file. But the local `swa` emulator does
not seem to check all the various rewrite rules etc, so when we actually
deployed it didn't work. We had to add a `route` attribute to the function.json
file, and then, again, the `swa` emulator did not check this correctly so there
was an error.

So there are some limitations with our testing strategy around the rewrite rules
that Azure will support; they don't seem to be validated locally by the emulator
in the same way they are validated in production. This could be due to the fact
that we are providing our own api server to the `swa` tool rather than allowing
it to redirect to serverless functions running locally. We did this for speed
and because there was no way we could control the serverless functions in the
way we need to for the tests if we went this route (at least I don't think there
is). 

So how can we fix this? We upgraded to the latest version of the `swa` emulator.
But that's not going to fix this problem since when we run that we provide our
own api server with hard coded paths etc. What we would need to do is run the
azure functions emulator and that would actually read the `function.json` files
etc and presumably do the right thing.

Note that when we run the function emulator, it does accurately validate all the
routing rules for the functions (like path params and validations etc).

Now, we could try to do this during the unit tests which would probably be ideal
but it's not clear how we would control the functions since the function
emulator expects to use the files that the function.json says to use. Note that
the main thing we control right now is the learning areas. For things like
setting up the app with a certain engagement plan, we use the CosmosRepository
to actually insert these into the fake cosmos directly. So, actually, if we
pulled learning areas from Cosmos, we could control everything we need to just
by interacting with the Cosmos DB ...

In any case, we could just start the function app emulator for each example,
like we do with the test server. We would have to build also though. The reason
we didn't try this before was just because the function app emulator didn't run
on Apple Silicon. It could work, but it's still pretty slow to start, much
slower than express.

Alteratively, maybe we need another kind of test? More like what I might call a
smoke test, ie one that runs the app with the function app tool and the swa tool
but with a fake cosmos db presumably. And it would test the routing for the
function apps?

But maybe the best thing is to REALLY have a smoke test that runs on the
deployed code in production and only make that code live if the smoke test
passes. That smoke test could log in a test user and see that certain things
have been selected. But that wouldn't really help our case unless we also tested
logging in from a learning area page, which feels like kind of a special case.
And it's not like the routing rules are going to pass or fail based on the
environment. Feels like I shuold be able to test those locally I guess? It's
just a matter of how much you trust the emulator I guess.

Potentially, alternatively, we have a kind of shim or adapter that the function
app emulator uses which proxies requests to our test adapters? But you'd have to
have some kind of cross-process communication basically somehow. Maybe we just
build the function apps with a LearningAreaRepository implementation that makes
a request to our test server to get the list of learning areas. I mean,
functions are supposed to be stateless, right? So why do we need to start and
stop them on every example? The trick here would be to build the functions with
test adapters. But I think that would just require providing a different entry
point since we already have entrypoints for all the functions except login.

Apparently the `swa` tool can do this for us too and since we already start
that, then it should work ok I guess so long as we can tell it the right
directly for the api. Tested this out and we can start the functions fine ...
just need to build and configure them for running locally against the fake
cosmos.

Now, I can build the functions using an entrypoint for a local deploy and it
works fine. The only problem now is referencing the HTML templates. The
functions expect these to be in their function context folder. We could copy
them at the beginning of the script, but it's not so great to have those files
in that location; it's just confusing to have them sitting there. We could make
an environment variable but I don't know what we would set it to for real. And
it would be messy to have a conditional on whether to use the environment
variable. We could always have the html files there but then we'd need to change
the root for Vite and that would mess things up I think maybe. We could also
store the html file in Azure blob storage and somehow emulate that locally.
Seems like too much though.

For now, I'll just go with an environment variable that we set locally. And if
it's not set, then it uses the function's directory via the executionContext.

So now we can run locally using the Azure functions emulator! The next step is
to get this working with the tests ...


### Getting the testing working with azure functions emulator

There's actually a few parts to start up.

- CosmosDB
- The browser (ie playwright)
- vite (this is new)
- the `swa` emulator (which starts the function emulator and of course the app)
- some way to communicate with the learning area repo
- (And we no longer need to run an express server)

Note that the last thing is the main question ... but maybe it doesn't need to
be a separate process (hopefully). We could use CosmosDB and just store a
document with the full list there. Then the test adapter would need to go in and
fetch from cosmos each time I guess. ... unless Cosmos has a way to subscribe to
changes. (It does but it looks kind of complicated)

I've started things up and we get the web page now during the test. But it's
failing since we're not yet able to control the learning areas. What's the best
way to do this?

We tried using IPC with the swa process ... but I bet that swa actually starts
another process to run the function server or in any case it doesn't look like
our function is receiving the ipc message.

We could also write a message to a file and have our function read the file when
it needs to get the learning areas? This works but doesn't seem that ideal.
Feels like it's better to not mess with the filesystem during the test but
starting an http server or something would be more work to set up.

Note that what we are doing now is programmatically starting a server and
providing the code for it to run at runtime. What I really want is just a shared
memory cache basically. But our fake cosmos db can do that for us.

We probably need to also start the functions emulator independently of the swa
so that we can kill it ... it doesn't seem to be killed now at the end of the
tests, only the swa is. And for that we could use the concurrently JS api ...

Or ... with esmodules you can actually import from a url ... so we could import
a module that our test server is serving. Problem is that this would be cached
the first time it was loaded and we would need it to change for each example.
But you could do a dynamic import. We would still need an express server in our
tests that could serve the module. This is really the same though as just making
an HTTP request ...

We set this up with an HTTP server and it works just fine ... use node-fetch to
make the request.

But now we need to figure out how to start everything up ...

Lots of stuff is related to azure, but some stuff seems to connect with the
tests ... like the engagementPlanRepo ... it has a reset method. Maybe our tests
need to somehow accept some dependencies. Note that the tests don't actually
need an EngagementPlanRepo ... they just need a way to write to the db and they
use the EngagementPlanWriter interface to do so. But they also need to reset
things between examples.

But they also need to start a server to provide learning areas, and that seems
maybe to be peculiar to azure I guess. 

This is starting to merge into the goal of making the test suite agnostic of the
deployment environment/strategy. Maybe we should just get this working and then
we can figure out how to abstract everything away.

Ok -- finally got it working. There were lots of problems getting the tests to
work in CI because they seemed to hang and never let the job finish. This is
because the behavior with `child_process.spawn` is different in linux than
MacOS. To be able to kill the process (which probably spins up child processes
too) on linux, I had to set the process to be `detached` (which makes the
process the leader of a group of processes) and then either inherit or ignore
the io. When I piped the io to process.stdout etc this seemed to keep the
process alive even after killing it, maybe. Anyways, now it works.


### Further distinguish the Azure specific stuff

We should probably also move the `local` directory inside Azure since it's all
specific to running the azure stuff locally. And our tests depend on Azure-specific adapters (like Cosmos and the local server) so is there a way to make it so they care less about this stuff?

Note that once we finish getting the tests to use the azure functions emulator,
we should be able to delete the `local` directory entirely, since we'll have
moved it inside the `azure` directory ... which is what we want.

What does the test suite need though? What adapters?

1. Something that allows for setting the learning areas during an example -- and
clearing them at the end?
2. Something that allows for setting the engagement levels during an example and
clearing them at the end of the example.

It's like these are the aspects of the outside world we need to control.
Eventually we could probably do other things, like simulating connection
failures or other errors. But for now, these are the only things that need to be
controlled. BUT, there's also authentication ... that is Azure specific as well
(the way that we actually log in). So really we need three things:

1. Something that allows for setting the learning areas during an example -- and
clearing them at the end?
2. Something that allows for setting the engagement levels during an example and
clearing them at the end of the example.
3. Something that allows for authenticating a user, ie logging the user in.

For authentication, it's kind of weird because right now we actually need to
interact with the browser to do this. So we can't be completely independent, we
need to be able to interact with the page ... and yet other deployments probably
wouldn't operate this way. So it's unclear what the interface should be for such
an 'adapter'. 

Note: We did move everything, including the `terraform` directory (which is also
Azure specific) under the `azure` directory. Eventually, if we have another
deployment option, we'll also need to move some aspects of the test suite into
the deployment-specific directory as well.


### Another feature: Reading Notes

Adding notes involves a series of features that will follow a basic CRUD
pattern. We begin by writing a story that says, basically, "When there are notes
in the database, then I see those displayed on the screen". This allows someone
to add notes via the Azure portal in production. Certainly not the best way to
add notes, but it's a way to get started without having to do both note creation
and note display at once, from the UI to the database.

In order to accomplish this, though, we need to have our *tests* be able to add
notes to the (test) database. We don't want to do this 'by hand', ie by talking
to Cosmos DB directly. Instead, we will actually write *part* of the code that
will be needed to create notes, namely, a `EngagementNoteWriter` interface and
an implementation for writing notes to Cosmos DB. Right now, this production
code will only be called by the tests. But when we do the feature to create
notes we can use it there to write notes to the database.

This strategy allows us to get something deployed a little faster, and protects
our test suite from change, since it just uses an abstraction to talk to the
database, the same abstraction the code will use to do so as well (eventually). 

We ended up writing a test that shows two logged in users, each seeing their own
notes for a learning area, and for another learning area seeing no notes.

We also need a test that shows that if you are not logged in at all you see
no notes as well.


### Creating Notes

Now that we're able to read notes, we want users to be able to create them
through the UI. With the current 'framework' we are building up to manage the
display, we are following the Elm architecture, where we have state stored in
one model and the view is a function of that model. But now we're getting to a
situation where we really want to have a notion of 'local state'.

In Elm, when you start to deal with an input field, you need to create an
attribute on your model to store the current value of that input field (there
are alternatives but this is the standard way to do it). This has always felt
kind of annoying to me. The model feels like it should hold 'important' state.
But the value of the text field is really just 'intermediate' state -- it only
becomes important once you save it, usually.

React, of course, has had different options for handling state local to a
component: class components that could store state, and now `useState` hooks.
React kind of encourages you to handle all state as local (component) state, and
it's only redux that asks you to follow something more like the Elm
architecture. In any case, the problem with React, I think, is that it goes too
far in allowing you to create local state in ways that eventually muddy the
separation of concerns in the app. A `useEffect` hook might make an HTTP
request and store the result with another `useState` hook. But now you've got
http requests in your view components, which feels gross.

React also has the notion of a `context` which is basically like a wrapper of
components that can make state available to that collection of components that
it wraps (via another hook). But again, the problem here is that you can set
values on the context in too many ways.

We introduced the notion of a `context` that's kind of like hooks in react but
only allows you to set state when it's the result of an even on some HTML
element. In other words, there's no API to arbitrarily set state or arbitrarily
'dispatch' a message. This allows us to do *just enough* to accomplish the task
we want, namely, keeping track of 'intermediate' state and then sending it up to
the main model when it becomes important.

So for our textarea that allows us to input the text of a new note. We wrap this
in a context, which provides the current state and a function to *generate a
message with the new state* which can then be dispatched via an HTML element
event handler, like `onInput` or `onClick` etc. When such a message is
dispatched, we re-render the view with the updated state, but don't send the
message through the main update function. When the `save note` button is
clicked, now the value of the text field is important, and we can use the
current state value to get that and send it in a message that does go to the
main update function. Using this pattern, we're still able to do cool things
like disable the save button when the value of the local state (the value of the
textarea) is the empty string.

A context in our framework can cover as many elements as one might want,
allowing those elements to share intermediate (local) state without needing to
add lots of fields to the model. This should make dealing with form inputs more
straightforward. And in general it seems like kind of a nice pattern.


### Dates

Now we have a feature to store the date that a note was created and display that
on the UI. How do we want to represent dates? What library if any should we use?

For deploying to Azure, we are using Cosmos DB and Cosmos recommends storing
dates in ISO8601 string format in the UTC timezone, and then storing an offset
separately if you need timezone information.

I think it's ok if we simply store the date in UTC format in the database and
display it in whatever the local timezone is. We really want to display dates
only in any case, but we'll store the time stamp as well. We'll use just the
standard `.toISOString()` function on the Date class to accomplish this. It
always outputs the date in UTC. 

As far as formatting goes, I believe we do need to use a library for that. We
will use the `date-fns` library as it seems more modular than others. Luxon is
another alternative; moment.js seems to basically be deprecated.

The other thing we have to think about is how to control time during our tests.

The first test we write adds to the test that assumed there were notes in the
database and then showed them displayed on the screen -- a 'read' test. We can
do this easily since we can just specify that the date we want when we put the
note in the database and see that it appears on the screen in the right format.

It's trickier once we write the test that shows: when a new note is created, it
is saved with the proper creation date (the current date) and that date is then
displayed when the note is displayed. For that, we need to actually control the
time during the test to make sure we know what the current date actually is.

We are using Playwright as the browser context in which our app runs during the
tests. The recommended way right now to mock the date is to add an 'init script'
that runs on every page (before other scripts on the page). It will load sinon,
initialize the fake timers, and then set the date to what we want it to be.

See [this issue](https://github.com/microsoft/playwright/issues/6347) for a discussion.

This seems to work ok -- we add another `fact` that sets the date on our test
context, and if that date is set when we create the browser page, then we add
the proper init script. We tried a few different approaches to this, but this
seems to work the best.